---
layout: post
title:  "Why We Need Sparse Algorithms"
subtitle: "Lessons in QR Decomposition"
date: "2025-02-25 12:27 -0500"
categories: [numerical methods, linear algebra]
tags: python matlab sparse
reading_time: 20
excerpt: See what goes wrong when we use a dense algorithm to factor a sparse
    matrix. 
---

I have been working through [my own implementation][csparse_pp] of the CSparse
routines from Tim Davis's [SuiteSparse][suitesparse] package, as described in
his [book][davis_book] and [YouTube lectures][davis_youtube]. In Chapter 5 of
the book, he describes the following experiment:

<!-- <a href="#fig:initial" data-reference-type="ref" data-reference="fig:initial">Figure 1</a>. -->
<figure>
<img src="{{ '/assets/images/sparseqr/davis_west0479_code.png' | absolute_url }}"
id="fig:davis_west0479_code"/>
<figcaption><span class="fig_number">Figure 1</span>.
The MATLAB code from the book to factor the sparse matrix <code>west0479</code>
using the QR decomposition.
</figcaption>
</figure>

For my own implementation, I have been writing the CSparse routines in C++, and
wrapping them in Python using pybind11. I have been testing the routines
against the MATLAB code that Davis provides in his book,
<span class="marginnote-wrapper"><label for="sn-0" class="margin-toggle">&#8862;</label>
<input type="checkbox" id="sn-0" class="margin-toggle"/>
<span class="marginnote">
I run the open-source Octave, but the results should be interchangeable with MATLAB.
</span></span>
as well as the relevant SciPy packages. 

When I tried to run this experiment, however, I ran into a problem. The
matrices `Q` and `V` that I generated had 114,811 and 21,791 non-zeros,
respectively, which was a far cry from the 38,070 and 3,906 non-zeros that
Davis reported (and MATLAB produced). The rest of this post explores what went
wrong and how I fixed it.


# The MATLAB Results

The matrix `west0479` is a 479-by-479 matrix with 1,888 non-zero entries (the
context is described in this [MATLAB blog][west0479_blog]). 

I'll make a slight tweak to the code from
<a href="#fig:davis_west04790_code" data-reference-type="ref" data-reference="fig:davis_west04790_code">Figure 1</a>
and call the matrix `A` instead of `west0479`. We can plot the sparsity pattern
using MATLAB's `spy` function. Davis uses the `colamd` function to reorder the
columns of `A` using the approximate minimum degree ordering. More on that
topic in a future post, but for now, know that it is a reordering that helps
reduce the additional number of non-zeros (known as *fill-in*) in the factors
of the matrix. The following code will plot the original `A`, and its reordered form:

```matlab
load west0479
A = west0479;
q = colamd(A);
Aq = A(:, q);
spy(A)
spy(Aq)
```

<a href="#fig:matlab_A" data-reference-type="ref" data-reference="fig:matlab_A">Figure 2</a>
shows the plots side by side. We can see that the reordering has a significant
impact on the structure of the matrix, but the number of non-zeros remains the
same.

<figure>
<img src="{{ '/assets/images/sparseqr/west0479_COLAMD_A_MATLAB.png' | absolute_url }}"
id="fig:matlab_A"/>
<figcaption><span class="fig_number">Figure 2</span>.
The sparsity pattern of the original matrix <code>A</code> and the reordered
matrix <code>A(:, q)</code> using the COLAMD algorithm.
</figcaption>
</figure>

<!-- TODO link to my blog post on Householder vectors -->
We can also take the QR decomposition of each of the matrices to see the
effects of the reordering. Further, we're going to use the CSparse function
`cs_qr` to get the matrix of Householder vectors `V`, their corresponding
coefficients `beta`, and a row permutation `p`. In many applications,
especially sparse ones, we don't actually want to compute the full `Q` matrix,
because it is often much denser than the original matrix. Instead, we can use
the sparse `V` matrix and the `beta` coefficients to apply the Householder
vectors to, say, a right-hand side matrix `B`. This point is exatly the one
Davis makes in his excerpt about this experiment.

```matlab
[Q, R] = qr(A);
[Qq, Rq] = qr(Aq);
[V, beta, p] = cs_qr(A);
[Vq, betaq, pq] = cs_qr(Aq);
```

Since `V` is guaranteed to be lower triangular, and `R` is upper triangular,
we'll plot them on the same `spy` plot.
<a href="#fig:matlab_QR_natural" data-reference-type="ref" data-reference="fig:matlab_QR_natural">Figure 3</a>
shows the sparsity pattern of the QR decomposition of the original matrix `A`,
and
<a href="#fig:matlab_QR_colamd" data-reference-type="ref" data-reference="fig:matlab_QR_colamd">Figure 4</a>
shows the sparsity pattern of the QR decomposition of the reordered matrix `A(:, q)`.

<figure>
<img src="{{ '/assets/images/sparseqr/west0479_NATURAL_QR_MATLAB.png' | absolute_url }}"
id="fig:matlab_QR_natural"/>
<figcaption><span class="fig_number">Figure 3</span>.
The sparsity pattern of the QR decomposition of the original matrix <code>A</code>.
</figcaption>
</figure>

<figure>
<img src="{{ '/assets/images/sparseqr/west0479_COLAMD_QR_MATLAB.png' | absolute_url }}"
id="fig:matlab_QR_colamd"/>
<figcaption><span class="fig_number">Figure 4</span>.
The sparsity pattern of the QR decomposition of the reordered matrix 
<code>A(:, q)</code>.
</figcaption>
</figure>

We can see that the reordering has a significant impact on the sparsity pattern,
as well as the number of non-zeros in the `Q` and `V` matrices. We can also see
that the `Q` matrix is substantially denser than the `V` matrix in both cases.
In fact, it has an order of magnitude more non-zeros than the `V` matrix!

# The Python Results
With Davis's point illustrated in MATLAB, I set about implementing the same
experiment in Python. The matrix is not inherently available in SciPy, so
I wrote it to a file and read it into a numpy array. In order to keep the
experiment consistent, I also wrote the `colamd` permutation vector `q` to
a file, and read that in as well.

## Aside: Computing COLAMD in Python

# The Fix

# Conclusions


<p class="message" markdown=1>
The entire source code for the figures and algorithms in this post is available
on GitHub: [python][python_source] and [MATLAB][matlab_source].
</p>


<!-- links -->
[suitesparse]: https://github.com/DrTimothyAldenDavis/SuiteSparse
[davis_youtube]: https://www.youtube.com/watch?v=1dGRTOwBkQs
[davis_book]: https://epubs.siam.org/doi/book/10.1137/1.9780898718881
[west0479_blog]: https://blogs.mathworks.com/cleve/2020/10/25/notes-on-cr-and-west0479/?s_tid=srchtitle_support_results_1_west0479#4c86b6bc-975d-4d4d-b46a-8918c1363e8c
[csparse_pp]: https://github.com/broesler/CSparse_pp
[python_source]: https://github.com/broesler/CSparse_pp/blob/main/python/scripts/west0479_experiment.py
[matlab_source]: https://github.com/broesler/CSparse_pp/blob/main/MATLAB/west0479_experiment.m 
<!-- [householder_blog]: post_url 2025-02-20-householder-vectors %} -->
